# -*- coding: utf-8 -*-
"""Sistem Rekomendasi Buku

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18SaOkq7Edxs6PLtmLvYpiv339Vzxp0MX

# Load the dataset
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d arashnic/book-recommendation-dataset

!unzip book-recommendation-dataset.zip

"""Ada 3 file yang akan kita gunakan pada pembuat sistem rekomendasi buku kali ini yaitu 'Books.csv', 'Ratings.csv' dan 'Users.csv'

# Data Understanding
"""

import pandas as pd

books = pd.read_csv("Books.csv")
ratings = pd.read_csv("Ratings.csv")
users = pd.read_csv("Users.csv")

ratings.info()

books.head()

ratings.head()

users.head()

print('Jumlah Buku: ', len(books.ISBN.unique()))
print('Jumlah Rating: ', len(ratings.ISBN.unique()))
print('Jumlah Pengguna: ', len(users['User-ID'].unique()))

"""# Univariate Exploratory Data Analysis

## Books variabel
"""

books.info()

jumlah_judul_buku = books['Book-Title'].nunique()
jumlah_ISBN = books['ISBN'].nunique()

print(f"Jumlah judul buku: {jumlah_judul_buku}")
print(f"Jumlah ISBN: {jumlah_ISBN}")

"""- Jumlah ISBN: 271.360
ISBN (International Standard Book Number) adalah nomor identifikasi unik untuk setiap versi atau edisi buku. Satu judul buku bisa memiliki beberapa ISBN jika ada berbagai versi atau edisi dari buku tersebut, seperti versi hardcover, paperback, terjemahan bahasa lain, atau edisi cetak ulang. Oleh karena itu, jumlah ISBN biasanya lebih banyak daripada jumlah judul buku, karena setiap variasi edisi dianggap sebagai entri yang terpisah.
- Jumlah Judul Buku (Book-Title): 242.135
Ini menunjukkan jumlah judul buku yang berbeda. Satu judul bisa muncul lebih dari sekali dengan ISBN yang berbeda karena variasi versi buku tersebut.
"""

jumlah_penulis = books['Book-Author'].nunique()
print(f"Jumlah Penluis Buku: {jumlah_penulis}")

"""Terdapat 102.024 jumlah penulis dari 271.360 jenis ISBN buku yang berbeda dan terdapat beberapa buku yang ditulis oleh penulis yang sama.

## Ratings variabel
"""

ratings.info()

ratings.describe()

jumlah_rating = ratings['ISBN'].nunique()

print(f"Jumlah Rating: {jumlah_rating}")

# Filter buku yang diberi rating (rating > 0)
buku_diberi_rating = ratings[ratings['Book-Rating'] > 0]

# Hitung jumlah ISBN unik yang memiliki rating
jumlah_buku_diberi_rating = buku_diberi_rating['ISBN'].nunique()

print(f"Jumlah buku yang diberi rating: {jumlah_buku_diberi_rating}")

"""Jumlah rating mencapai 340.556, sementara hanya 185.973 buku yang diberi rating lebih dari 0, menunjukkan bahwa tidak semua buku mendapatkan rating dari pengguna.

## Users variabel
"""

users.info()

print('Banyak pengguna:', len(users['User-ID'].unique()))
print('Lokasi pengguna:', users['Location'].unique())

"""# Data Preprocessing

## Menggabungkan file buku dan rating berdasarkan nomor ISBN
"""

merged = pd.merge(ratings, books, on='ISBN', how='left')

print("Jumlah data merged:", len(merged))
merged

"""# Data Preparation

## Mengecek Missing Values
"""

users.isnull().sum()

users.dropna(inplace=True)

users.isnull().sum()

merged.isnull().sum()

merged.dropna(inplace=True)

merged.isnull().sum()

# Membuat variabel preparation yang berisi dataframe fix_resto kemudian mengurutkan berdasarkan placeID
preparation = merged
preparation.sort_values('ISBN')

# Membuang data duplikat pada variabel preparation
preparation = preparation.drop_duplicates('ISBN')
preparation

# Mengonversi data series ‘ISBN’ menjadi dalam bentuk list
rating = preparation['Book-Rating'].tolist()

# Mengonversi data series ‘Name’ menjadi dalam bentuk list
books_name = preparation['Book-Title'].tolist()

books_isbn = preparation['ISBN'].tolist()


print(len(rating))
print(len(books_name))
print(len(books_isbn))

data_new = pd.DataFrame()
data_new['ISBN'] = books_isbn
data_new['Book-Title'] = books_name
data_new['Book-Rating'] = rating
data_new

"""# Model Development dengan Content Based Filtering"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

data = data_new
data.sample(5)

# Gunakan hanya 10.000 baris pertama
data_cut = data_new.head(10000)

# Konversi kolom judul buku ke dalam bentuk list
books_title_list = data_cut['Book-Title'].tolist()

# Simpan data hasil preprocessing ke dalam variabel books_new
books_new = data_cut.copy()

"""## TF-IDF Vectorizer"""

# Inisialisasi TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words='english')

# Transformasi judul buku ke bentuk matriks TF-IDF
tfidf_matrix = tfidf.fit_transform(books_title_list)

# Konversi ke bentuk dense matrix
tfidf_dense = tfidf_matrix.todense()

print("Shape of TF-IDF Matrix:", tfidf_matrix.shape)

"""## Cosine Similarity"""

# Hitung kemiripan antar judul buku
cosine_sim_matrix = cosine_similarity(tfidf_matrix)

print("Shape of Cosine Similarity Matrix:", cosine_sim_matrix.shape)

# Konversi cosine_sim_matrix jadi DataFrame
cosine_sim_df = pd.DataFrame(
    cosine_sim_matrix,
    index=books_new['ISBN'],
    columns=books_new['ISBN']
)

"""## Mendapatkan Rekomendasi"""

def book_recommendations_by_isbn(isbn_buku, similarity_data=cosine_sim_df, items=books_new[['ISBN', 'Book-Title', 'Book-Rating']], k=5):
    # Cek apakah ISBN ada di kolom similarity
    if isbn_buku not in similarity_data.columns:
        return f"ISBN '{isbn_buku}' tidak ditemukan dalam data."

    # Ambil index dengan similarity tertinggi
    index = similarity_data.loc[:, isbn_buku].to_numpy().argpartition(range(-1, -k, -1))

    # Ambil ISBN yang paling mirip (tertinggi kemiripannya)
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Hapus ISBN input supaya nggak direkomendasiin dirinya sendiri
    closest = closest.drop(isbn_buku, errors='ignore')

    # Gabung dengan data buku agar dapat judul dan rating
    return pd.DataFrame({'ISBN': closest}).merge(items, on='ISBN', how='left').head(k)

books_new[books_new['ISBN'] == '0449143600']

book_recommendations_by_isbn('0449143600')

"""# Model Development dengan Collaborative Filtering"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

# Load Data
books = pd.read_csv("Books.csv")
ratings = pd.read_csv("Ratings.csv")

"""## Data Preparation"""

merged = pd.merge(ratings, books, on='ISBN', how='left')
merged.dropna(inplace=True)

df = ratings.head(10000).copy()

# Encoding user dan isbn
user_ids = df['User-ID'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

isbn_ids = df['ISBN'].unique().tolist()
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_ids)}
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_ids)}

# Mapping encoded id
df['user'] = df['User-ID'].map(user_to_user_encoded)
df['books'] = df['ISBN'].map(isbn_to_isbn_encoded)

# Konversi rating ke float
df['Book-Rating'] = df['Book-Rating'].values.astype(np.float32)

# Normalisasi rating 0-1
min_rating = df['Book-Rating'].min()
max_rating = df['Book-Rating'].max()

"""## Membagi Data untuk Training dan Validasi"""

# Membuat variabel training dan validation
x = df[['user', 'books']].values
y = df['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Split data
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Proses Training"""

# Model RecommenderNet
class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size=50, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(num_users, embedding_size,
                                               embeddings_initializer='he_normal',
                                               embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.user_bias = layers.Embedding(num_users, 1)
        self.books_embedding = layers.Embedding(num_books, embedding_size,
                                                embeddings_initializer='he_normal',
                                                embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.books_bias = layers.Embedding(num_books, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        books_vector = self.books_embedding(inputs[:, 1])
        books_bias = self.books_bias(inputs[:, 1])
        dot_user_books = tf.tensordot(user_vector, books_vector, 2)
        x = dot_user_books + user_bias + books_bias
        return tf.nn.sigmoid(x)

# Compile model
num_users = len(user_to_user_encoded)
num_books = len(isbn_to_isbn_encoded)

model = RecommenderNet(num_users, num_books)
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Train model
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=64,
    epochs=100,
    validation_data=(x_val, y_val),
    verbose=1
)

# Plot RMSE
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Model Metrics')
plt.ylabel('RMSE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')
plt.show()

"""## Mendapatkan Rekomendasi Buku"""

# Mengambil sample user
user_id = ratings['User-ID'].sample(1).iloc[0]
books_visited_by_user = ratings[ratings['User-ID'] == user_id]

# Pilih random user_id yang memang ada di user_to_user_encoded
user_id = np.random.choice(list(user_to_user_encoded.keys()))

# User encoder dijamin ada
user_encoder = user_to_user_encoded.get(user_id)

# Data buku yang belum dikunjungi
books_not_visited = merged[~merged['ISBN'].isin(ratings[ratings['User-ID'] == user_id]['ISBN'].values)]['ISBN']
books_not_visited = list(set(books_not_visited).intersection(set(isbn_to_isbn_encoded.keys())))
books_not_visited = [[isbn_to_isbn_encoded.get(x)] for x in books_not_visited]

# Buat array prediksi
user_books_array = np.hstack(([[user_encoder]] * len(books_not_visited), books_not_visited))
user_books_array = np.array(user_books_array).astype(np.int32)

# Prediksi rating
ratings_pred = model.predict(user_books_array).flatten()

# Top rekomendasi
top_ratings_indices = ratings_pred.argsort()[-10:][::-1]
recommended_books_ids = [isbn_encoded_to_isbn.get(books_not_visited[x][0]) for x in top_ratings_indices]

print(f'Showing recommendations for user: {user_id}')
print('='*30)
print('Books with High Ratings from User:')
print('-'*30)
top_books_user = (
    books_visited_by_user.sort_values(by='Book-Rating', ascending=False)
    .head(5)
    .ISBN.values
)

for isbn in top_books_user:
    if isbn in books['ISBN'].values:
        title = books[books['ISBN'] == isbn]['Book-Title'].values[0]
        rating = books_visited_by_user[books_visited_by_user['ISBN'] == isbn]['Book-Rating'].values[0]
        print(f"{title}: {rating}")


print('-'*30)
print('Top 10 Book Recommendations:')
print('-'*30)

for isbn in recommended_books_ids:
    title = books[books['ISBN'] == isbn]['Book-Title'].values[0]
    print(title)

def show_recommendations(recommended_books_ids, books_df):
    for isbn in recommended_books_ids:
        if isbn in books_df['ISBN'].values:
            title = books_df[books_df['ISBN'] == isbn]['Book-Title'].values[0]
            print(f"{isbn} - {title}")
        else:
            print(f"{isbn} - Buku tidak ditemukan dalam dataset.")

# Mengecek apakah buku ada di dataset:
show_recommendations(recommended_books_ids, books)